ansible-galaxy collection install community.general

# C4.D1 Задания D1.3.3, D1.7 (Итоговое задание)

## Задание

Оригинальная формулировка задания приведена [здесь](./TASK.md).

## Примечания к решению

- Исходя из условий заданий одного решения на два задания - достаточно.
- Согласно условиям задания D1.3.3 "закат Солнца вручную" - не требуется, и всё задание можно выполнить двумя командами (конкретные аргументы приведены ниже): ansible-galaxy (роль работы с docker swarm) и ansible (непосредственно "раскатка" и масштабирование приложения).
- Согласно условиям задания D1.7 решение - может быть на основе кластера docker swarm.
- Логин, пароль доступа к базе передаются в переменной среды окружения (зашифрованный vault, пароль - 1 - в файле, исключенном из сохраняемых файлов репозитория; файл подключен в [конфигурации ansible](./ansible.cfg)) в соответствии с логикой самого приложения, предложенного к установке.
  Процесс выполнен сугубо в учебных целях для задачи с иными целями демонстрации, отличными от выполнения
  лучших практик по безопасности инфмормации
- Внешний адрес запрашивается только для хоста-менеджера/бастиона. Доступ на иных (рабочих) хостах к ресурсам Internet (пакеты apt, docker) - через прокси на бастионе.
- Доступ к рабочим хостам - через бастион по ssh.
- Сервисы прокси (на бастионе), докера - "слушают" на внутренних ip-адресах сетевых интерфейсов (не 0.0.0.0).
- Узел управления (единственный по условию заданий) намеренно (т.к. совмещается с бастионом согласно условию заданий) исключен из процедуры планирования запуска контейнеров (переведен в режим drained).
- Изменение конфигурации приложения (docker-compose.yml) при повторной накатки роли соответственно отображается для кластера (в частности, остановкой-запуском измененных сервисов) в листинге ps как события с ненулевым завершением.
- Изменение числа реплик сервиса соответственно отображается для кластера в листинге ps как события с ненулевым завершением.
- Контейнер-сервис app0_user-sim - короткоживущий согласно логике приложения, перезапуск - оставлен в соответствии с логикой приложения.

## Ход решения (принципиальная схема развертывания)

0. Устанавливаем локально поддержку community.docker для работы с docker swarm
```
ansible-galaxy collection install community.docker
```

1. Правим настройки создания сервера в Terraform, развертывания приложения (блок vars)
```
vi ansible-0-provision-playbook.yml
```

2. Создаем серверы для решения [задания](./TASK.md), автогенерируем [инвентарь ansible](./ansible-1-deploy-inventory.txt) и [настройки ssh-подключения](./ansible-1-deploy-ssh.config) для созданных хостов следующего шага
```
ansible-playbook -i ansible-0-provision-inventory.txt ansible-0-provision-playbook.yml  | tee out/ansible-0-provision-apply.log
```

3. Настраиваем серверы, развертываем кластеры, развертываем приложение в соответствии с критериями [задания](./TASK.md)
```
ansible-playbook -i ansible-1-deploy-inventory.txt ansible-1-deploy-playbook.yml | tee out/ansible-1-deploy-apply.log
```

4. Тестируем работоспособность приложения и управляющего кластера в соответствии с критериями [задания](./TASK.md)
```
ansible-playbook -i ansible-1-deploy-inventory.txt ansible-2-test-playbook.yml | tee out/ansible-2-test-apply--1node.log

```

5. Масштабируем сервис front-end до двух экземпляров (настройка - в переменных [плейбука](./ansible-3-scale-playbook.yml)) в соответствии с критериями [задания](./TASK.md)
```
ansible-playbook -i ansible-1-deploy-inventory.txt ansible-3-scale-playbook.yml | tee out/ansible-3-scale.log
```


6. Тестируем работоспособность приложения и управляющего кластера в соответствии с критериями [задания](./TASK.md)
```

ansible-playbook -i ansible-1-deploy-inventory.txt ansible-2-test-playbook.yml | tee out/ansible-4-test-scale.log

```

7. Итоги развертывания:
- скриншот работающего сайта - [здесь](https://github.com/taa2021/c4-d1-3/blob/main/out/site-screenshot.png);
- диагностики кластера после развертывания и масштабирования - [здесь](https://github.com/taa2021/c4-d1-3/blob/main/out/docker-screenshot.png);
- ход развертывания серверов - [здесь](https://github.com/taa2021/c4-d1-3/blob/main/out/ansible-0-provision-apply.log);
- ход настройки серверов и развертывания приложения - [здесь](https://github.com/taa2021/c4-d1-3/blob/main/out/ansible-1-deploy-apply.log);
- результаты проверки правильности развертывания приложения - [здесь](https://github.com/taa2021/c4-d1-3/blob/main/out/ansible-2-test-apply.log);
- ход масштабирования приложения - [здесь](https://github.com/taa2021/c4-d1-3/blob/main/out/ansible-3-scale.log);
- результаты проверки правильности масштабирования приложения (см. "check docker node ps..." ) - [здесь](https://github.com/taa2021/c4-d1-3/blob/main/out/ansible-4-test-scale.log);

## Выводы по решению

- Хосты в Яндекс.Облаке (далее - vm) - развернуты через IaC-описание (terraform).
- Проект на хостах - развернут через IaC-описание (ansible).
- Проект - смаштабирован через IaC-описание (ansible).
- Скриншоты страницы в браузере(главной страницы проекта, работающего в Yandex.Cloud), диагностики кластера после развертывания, логи развертывания и тестирования - [приведены](https://github.com/taa2021/c4-d1-3/blob/main/out/);
- Описание каким образом и какие команды использовались для решения задания - приведены выше в разделе "Примечания к решению" данного описания
- Выводы команд dockerservicels и dockernodels (согласно формулировке задания - без пробелов; в решении команды изменены согласно смысловой нагрузке) - [приведены](https://github.com/taa2021/c4-d1-3/blob/main/out/) (см. файлы ansible*-test-*.log);
- Развертывание приложения, работа приложения, масштабирование приложения - протестированы с результатом: успешно.
